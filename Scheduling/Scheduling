Scheduling in kubernetes:

Manual Scheduling: 

Generally Scheduler will check for spec.nodeName in the pod definition file if its not present then it will schedule the pod in appropriate node
or we can manually mention the node details in nodeName field

- if scheduler is not present then Pod status will be in Pending state or you can manually assign pod to the nodes
- at the time of pod creation you can mention nodename property in pod.yml in spec.nodeName
- for existing pod whose status is pending beacuse it is wating for teh node to assign then you have to create a binding object yml and then send post request binding API
with binding data in json format
- you cant edit the nodeName of a pod in live environment

Labels and Selectors: --> used for grouping and filtering the objects
---------------------

Labels - are properties attached to each item
selectors - used to filter these item

#ku get po --selector app=app1

in order to connect replica sets to pod we can config selector field

a single label can do if it matches correctly
if you specify multiple labels then pod will be assigned to replicasets consisting of those multiple labels

Annotations: used to record othe details of info pupose - tool , build, contact,e mail details
metadata.annotations

Taints & Tolerations: it wont deals with security and intrusions
---------------------
Taint ise set on master node that prevents pod to be scheduled on this node
it will only restricts  nodes from accepting certain pods
if you req is restrict a pod on certain nodes it can be acheived with Node affinity
it doesnt tell pods to go to specific nodes instead it tells to  nodes that to accept pods with  certain tolerations

are used to set restrictions on what pod to be scheduled to a node

taints are set on nodes and tolerations are set to pods

Taint-Node

#kubectl taint nodes node-name key=value:taint-effect

we have three taint-effects i.e, what happens to pods that do not tolerate to taints
NoSchedule - pods will not be schedule on NOdes
PreferNoSchedule - system will try to avoid plcaing pod on a node but thats not guaranteed
NoExecute - new pods will not be scheduled on the node and existing pods will be evicted (pod will be kiled) if pods dont tolerate the taint

Tolerations - PODs

its mentioned in spec.tolerations in key: "value" format and value should encode with double quotes
Key,operator,value(Equal),effect fields shoold be updated in spec.tolerations
when the pods are created/updated with new tolerations they are either not scheduled on the nodes or evicted from the existing nodes 
=====================================================================================================================================================================================================

NodeSelectors:
--------------
We can set limitation of a pod so that it will be assigned to particular nodes.

in pod def file add spec.nodeSelector with size: Large

before proceeding with above step assign key-value pair size: Large to a node so that pod will be placed on this node with matching Labels

To Label a node:
#kubectl label nodes node_name key=value
Nodeselectors served our purpose but still it has limitations, we used a single label to achieve our goal but 
if req is complex 
i.e Size should be
Large or Medium
but not small
We cant provide advances expressions like ARE or NOT using NodeSelectors for the above requirement
then we use Node AFfinity & Anti Affinity feature
========================================================================================================================================================

NodeAffinity: main feature is ensuring pods are hosted on particular nodes 

uses advanced capability to limit pod placement on specific nodes

Operator:
a)Equal --> check both key and value pair to exist
b)Exists --> checks only key to exist and it doesnt check teh value
C)In --> it will check for any one of value 
d) NotIn --> it wont accept  any of the value 


If podAffinity doesnot match the given expression with Label called size or 
if pods are scheduled on a node and what if someone changes the Label on the Node

so, it depends on the Node Affinity Type property

the type of Node affinity defines the behaviour of the scheduler wrt to Node Affinity & stages in LIfecycle of a pod

TwoTypes:
Available:
a)requiredDuringSchedulingIgnoredDuringExecution
b)prefferedDuringSchedulingIgnoredDuringExecution

Planned:
a)requiredDuringSchedulingrequiredDuringExecution

There are two states in pod lifecycle when considering NodeAffinity

a)During Scheduling - pod doesnot exist and created for the first time. SO, Affinity rules are considered to place pods in right nodes

b)DuringExecution- pod has been running and change made in env affects node affinity

If podAffinity doesnot match the given expression with Label called size on Nodes then it depends on the Node Affinity Type property

During Scheuling is required --> pod will not be scheduled
During Scheduling is Preferred --> Pod will try to place on node with matching node affinity rules & if not present then it will  ignore Node Affinity and place on any available node

if pods are scheduled on a node and what if someone changes the Label on the Node

During execution is ignored--> Pod will be running in the same node, any changes in NodeAffinity wil not impace them once schedule them

During execution is Required --> which will evict the pod on the node that dont have matching affinity rules

In Taints & Tolerations there is chance that the desired pod might placed in another node
in Node affinity another set of pods might get  placed in our desired Nodes
So , we use combination of both to ensure specifc pod is placed in specific node

===============================================================================================================================================================================================


Resource Requirements & limitations:
=====================================

By default POD or POD/container requires 0.5 CPU 256 Mi - Resource req of a container
By default k8s will limit the resources to container i.e, CPU - 1 , Memory - 512 Mi


In Docker world , generally there is no limitations for a conatiner to consume on a node 


CPU - 0.5 or 500m --> #you can go as lower as 1m i.e, 1milli
1 CPU - 1 AWS vCPU
incase of CPU, K8s throttles the cpu so it doesnot go beyond the specified limit
incase of memory , it can uses more memory than its limit, pod will be terminated

Memory - 1Gi or 256 Mi or Ki

1 Gi= 1024 Mi
1 Mi= 1024 Ki
1Ki= 1024 bytes

For the POD to pick up those defaults you must have first set those as default values for request and
limit by creating a LimitRange in that namespace.

Limit-Range:
-----------

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
    
    ================================================================================================================================================================


Daemon sets:
-------------
its like replicasets and helps to deploy multiple instances of pod but it runs each copy of your pod in every node

whenever a new node was added to the cluster a replica of tehat pod will be added and when node is removed pod will be removed

At any point of time one copy of pod should present in daemonsets


Usecases:
---------

Deploy a monitoring agent or log collector on each node of your cluster to monitor better then daemonset will deploy monitoring agent in the form of pod in all nodes of cluster

how it schedules pod on each node
it uses nodeName field to place pod on each node in a cluster. until k8s v1.12

from v1.2 uses Node Affinity & Default Scheduler
===========================================================================================================================================================================
Static PODs:
Kubelet can manage a node Independently in worker node with out master node

you can config kubelet to read the pod def files in designation paths etc/kubernetes/manifests
To config a deignation folder 
update path details in --pod-manifest-path in kubelet.service

instead of specifying path in service file you can provide path to config file in kubelet.service
--config=kubeconfig.yaml


in the kubeconfig.yaml mention path
staticPOdPath: /etc/kubernetes/manifests



it ensures pod is live, if any cahnegs made in pod def file of aove folder kubelet recreates a pod for those change so effect

these pods which are created by kubectlet without intervention of kube-api or rest of the cluster components

you can create only PODS but not replicasets/deployments

kubeapi provides input as httpAPI endpoint to kubelet to create a pod in cluster
kubele can create a static from pod defintion file in the given config path

when kubelet creates a static pod  it will also create a mirror  object in kubeapi server - 
if you exec the cmd ku get pods it will display a static pod as well which was created by kubelet i.e, a read only mirror of the static pod


